# ğŸ’° Analyse du coÃ»t des tokens pour le chatbot

## ğŸ“Š 1770 tokens : Est-ce beaucoup ?

### Comparaison rapide
- **1770 tokens** â‰ˆ **1400-1500 mots** en franÃ§ais
- C'est **modÃ©rÃ©** pour une requÃªte avec contexte complet d'entreprise
- Pour comparaison :
  - Un email moyen : ~100-200 tokens
  - Un article de blog : ~500-1000 tokens
  - Un livre : ~100,000+ tokens

### CoÃ»t rÃ©el (GPT-4o-mini)

**Prix OpenAI GPT-4o-mini** (dÃ©cembre 2024) :
- **Input** : $0.150 par million de tokens
- **Output** : $0.600 par million de tokens

**CoÃ»t de votre requÃªte** :
- Si 1770 tokens = **input uniquement** : `1770 Ã— $0.150 / 1,000,000 = $0.00027` (0.027 centimes)
- Si mix input/output (80% input, 20% output) : `(1416 Ã— $0.150 + 354 Ã— $0.600) / 1,000,000 = $0.00038` (0.038 centimes)

**En euros** (taux approximatif 1â‚¬ = 1.10$) :
- **~0.0003â‚¬ par requÃªte** (0.03 centimes d'euro)
- **~1â‚¬ pour 3000 requÃªtes**
- **~10â‚¬ pour 30,000 requÃªtes**

### Est-ce normal ?

Oui, c'est **normal** pour votre cas d'usage car :

1. **Contexte riche** : Vous envoyez le contexte complet de l'entreprise :
   - Clients (liste + factures impayÃ©es)
   - Devis et factures rÃ©cents
   - TÃ¢ches (statuts + urgentes)
   - Projets actifs
   - Rendez-vous
   - Relances en attente
   - Conversations inbox rÃ©centes

2. **3 messages** : Le log indique "3 messages Ã  ChatGPT" :
   - Message systÃ¨me (contexte)
   - Historique de conversation (si prÃ©sent)
   - Message utilisateur actuel

3. **GPT-4o-mini** : Vous utilisez dÃ©jÃ  le modÃ¨le le moins cher d'OpenAI

### Optimisations possibles

#### 1. Limiter la quantitÃ© de donnÃ©es dans le contexte

**Actuellement** (dans `chatbot_context_service.py`) :
- Limite par dÃ©faut : `limit=20` Ã©lÃ©ments par catÃ©gorie
- Affiche jusqu'Ã  5 Ã©lÃ©ments dans chaque section

**Option A : RÃ©duire la limite**
```python
# Au lieu de limit=20, utiliser limit=10
context = await build_company_context(db, company_id, limit=10)
```

**Option B : Filtrer plus agressivement**
- Ne pas inclure les sections inutiles selon la question
- Charger dynamiquement le contexte selon le type de question

#### 2. Utiliser un systÃ¨me de cache de contexte

Cache le contexte pendant X minutes pour Ã©viter de le reconstruire Ã  chaque requÃªte :
- Contexte reconstruit toutes les 5 minutes
- RÃ©utilisÃ© pour toutes les requÃªtes dans cette fenÃªtre

#### 3. Contexte dynamique basÃ© sur l'intention

Analyser d'abord l'intention de la question, puis charger seulement le contexte pertinent :
- Question sur les factures â†’ Charger uniquement le contexte facturation
- Question sur les tÃ¢ches â†’ Charger uniquement le contexte tÃ¢ches
- Question gÃ©nÃ©rale â†’ Charger un rÃ©sumÃ© minimal

#### 4. RÃ©duire les dÃ©tails dans le formatage

Dans `chatbot_service.py`, ligne 77-78 :
```python
# Actuellement : Affiche email, nombre de factures, nombre de devis
lines.append(f"   - {client.get('name', 'Sans nom')} ({client.get('email', 'N/A')}) - {client.get('total_invoices', 0)} factures, {client.get('total_quotes', 0)} devis")

# OptimisÃ© : Juste le nom
lines.append(f"   - {client.get('name', 'Sans nom')}")
```

### Recommandations

#### Pour l'instant : **Laissez comme c'est**

Pourquoi ?
- Le coÃ»t est **nÃ©gligeable** (0.03 centimes par requÃªte)
- Le contexte complet amÃ©liore **significativement** la qualitÃ© des rÃ©ponses
- GPT-4o-mini est dÃ©jÃ  le modÃ¨le le **moins cher**
- L'optimisation prÃ©maturÃ©e peut **dÃ©grader l'expÃ©rience utilisateur**

#### Optimiser seulement si :

1. **Volume trÃ¨s Ã©levÃ©** : Plus de 1000 requÃªtes/jour/utilisateur
2. **CoÃ»t devient problÃ©matique** : DÃ©passement de budget mensuel
3. **Performances lentes** : Le temps de rÃ©ponse devient inacceptable

### Calcul du budget estimÃ©

**ScÃ©nario rÃ©aliste** :
- 10 utilisateurs actifs
- 20 questions/jour/utilisateur
- 1770 tokens/requÃªte en moyenne

**Calcul** :
- RequÃªtes/jour : 10 Ã— 20 = **200 requÃªtes/jour**
- RequÃªtes/mois : 200 Ã— 30 = **6000 requÃªtes/mois**
- CoÃ»t/mois : 6000 Ã— $0.0003 = **$1.80/mois** (~1.65â‚¬/mois)

**Conclusion** : MÃªme avec un usage intensif, le coÃ»t reste **trÃ¨s faible**.

### Suivi des coÃ»ts

Pour surveiller les coÃ»ts rÃ©els, vous pouvez :

1. **Activer le logging dÃ©taillÃ©** dans `chatbot_service.py` :
```python
logger.info(f"[CHATBOT] Tokens utilisÃ©s - Input: {response.usage.prompt_tokens}, Output: {response.usage.completion_tokens}, Total: {response.usage.total_tokens}")
```

2. **CrÃ©er une table de tracking** dans la base de donnÃ©es :
   - Enregistrer chaque requÃªte avec le nombre de tokens
   - Calculer le coÃ»t par utilisateur/entreprise
   - GÃ©nÃ©rer des rapports mensuels

3. **Configurer des alertes OpenAI** :
   - Dashboard OpenAI â†’ Billing â†’ Set spending limits
   - Alerte Ã  $5, $10, $50, etc.

---

**Conclusion** : 1770 tokens, c'est **normal et Ã©conomique** pour votre cas d'usage. Continuez Ã  optimiser la qualitÃ© plutÃ´t que les coÃ»ts tant que le budget reste raisonnable.
